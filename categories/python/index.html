
<!DOCTYPE html>
<html lang="zh-cn">
    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Lizany Li">
    <title>分类: python - Lizany Li</title>
    <meta name="author" content="Lizany Li">
    
    
        <link rel="icon" href="https://lizanyroger.github.io/assets/images/lo.png">
    
    
        <link rel="alternate" type="application/atom+xml" title="RSS" href="/atom.xml">
    
    <meta name="description" content="a coding jackaroo in the bottom half of puberty">
<meta property="og:type" content="blog">
<meta property="og:title" content="Lizany Li">
<meta property="og:url" content="https://lizanyroger.github.io/categories/python/index.html">
<meta property="og:site_name" content="Lizany Li">
<meta property="og:description" content="a coding jackaroo in the bottom half of puberty">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Lizany Li">
<meta name="twitter:description" content="a coding jackaroo in the bottom half of puberty">
    
    
        
    
    
        <meta property="og:image" content="https://lizanyroger.github.io/assets/images/glass.jpg"/>
    
    
    
    
    <!--STYLES-->
    <link rel="stylesheet" href="/assets/css/style-sostf8a1aivpmdlygot3rltry8aweydwovdzyxr97qzr9cf2eciqkhffzoth.min.css">
    <!--STYLES END-->
    
    
</head>

    <body>
        <div id="blog">
            <!-- Define author's picture -->


<header id="header" data-behavior="1">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <h1 class="header-title">
        <a class="header-title-link" href="/ ">Lizany Li</a>
    </h1>
    
        
            <a  class="header-right-icon "
                href="#about">
        
        
            <i class="fa fa-lg fa-glass.jpg"></i>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="1">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a href="/#about">
                        <img class="sidebar-profile-picture" src="/assets/images/glass.jpg"/>
                </a>
                <h4 class="sidebar-profile-name">Lizany Li</h4>
                
                    <h5 class="sidebar-profile-bio"><p>author.bio</p>
</h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/ "
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-home"></i>
                        <span class="sidebar-button-desc">首页</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-categories"
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
                        <span class="sidebar-button-desc">分类</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-tags"
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
                        <span class="sidebar-button-desc">标签</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="/all-archives"
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
                        <span class="sidebar-button-desc">归档</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link open-algolia-search"
                             href="#search"
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-search"></i>
                        <span class="sidebar-button-desc">搜索</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link "
                             href="#about"
                            
                        >
                    
                        <i class="sidebar-button-icon fa fa-lg fa-question"></i>
                        <span class="sidebar-button-desc">关于</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a  class="sidebar-button-link " href="https://github.com/lizanyRoger" target="_blank">
                    
                        <i class="sidebar-button-icon fa fa-lg fa-github"></i>
                        <span class="sidebar-button-desc">GitHub</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="1"
                 class="
                        hasCoverMetaIn
                        ">
                
    <section class="postShorten-group main-content-wrap">
    
    
    <article class="postShorten postShorten--thumbnailimg-bottom" itemscope itemType="http://schema.org/BlogPosting">
        <div class="postShorten-wrap">
            
            <div class="postShorten-header">
                <h1 class="postShorten-title" itemprop="headline">
                    
                        <a class="link-unstyled" href="/2016/12/14/一个编程小白白的爬虫&贝叶斯之旅（一）/">
                            一个编程小白白的爬虫&amp;贝叶斯之旅（一）
                        </a>
                    
                </h1>
                <div class="postShorten-meta">
    <time itemprop="datePublished" datetime="2016-12-14T14:48:47+08:00">
	
		    12月 14, 2016
    	
    </time>
    
        <span>发布在 </span>
        
    <a class="category-link" href="/categories/python/">python</a>


    
</div>

            </div>
            
                <div class="postShorten-content" itemprop="articleBody">
                    <p>作为一只纯纯小白，第一次接触python&amp;爬虫&amp;文本分类，算是绝对的push自己接触新技术了，刚开始很方，每做一步都会继续慌下一步，所幸一步步做下来回头看感觉还不错嘛~</p>
<h5 id="下面进入正题……"><a href="#下面进入正题……" class="headerlink" title="下面进入正题……"></a>下面进入正题……</h5><p>目标分阶段性有四：</p>
<ol>
<li>爬虫获取语料库</li>
<li>对语料库进行预处理（包括分词和去停用词）</li>
<li>构建数据词典（训练）</li>
<li>对测试集分类</li>
</ol>
<h4 id="第一步：爬虫获取语料库（也就是这篇文章的内容-）"><a href="#第一步：爬虫获取语料库（也就是这篇文章的内容-）" class="headerlink" title="第一步：爬虫获取语料库（也就是这篇文章的内容~）"></a>第一步：爬虫获取语料库（也就是这篇文章的内容~）</h4><h5 id="（1）搜狐新闻"><a href="#（1）搜狐新闻" class="headerlink" title="（1）搜狐新闻"></a>（1）搜狐新闻</h5><p>在这里本小白还属于python的helloworld水平，所以编出的程序报了many bugs，然后处于bug-debug死循环中…</p>
<p>还是先定一个小目标：爬取8类（国际、国内、财经、军事、科技、健康、娱乐、体育），每类约2万篇文章。</p>
<p>先说说我选取源网页的过程，因为水平有限，所以在选取网页的时候费了很多功夫就为了选一个页面结构清晰简单的网站（即每个页面的文章标题内容以及下一页url都写在同一class/id的html标签中，而不是靠js代码加载，页面有很规则的文章列表，有统一的展示标准，这使得批量化获取信息成为可能），于是最后终于让我找到了一个完美的网站——搜狐新闻（我好爱它，虽然它的数据量远不够大，但作为一个入门级小白练手的网站简直再标准不过了）</p>
<p>首先，它的二级页面中，各篇文章的容器为<code>class=”article-list”</code>的<code>div</code>，文章详细页面的url在<code>&lt;div class=”article”&gt;</code>的子节点<code>&lt;h3&gt;</code>的子节点<code>&lt;a&gt;</code>的<code>href</code>属性中。</p>
<p><img src="../../../../assets/images/blogs/1.png" alt="img"></p>
<p>来看看如何翻页，这次就有点麻烦了，并没有url，而是onclick执行js的函数，</p>
<p><img src="../../../../assets/images/blogs/2.png" alt="img"></p>
<p>查看网页源代码可以看到<code>go(page)</code>的JavaScript函数，然而，虽然看起来非常复杂，但是其实他的意思就是，下一页就是url中的数字减一，减到等于0就没有下一页了</p>
<p><img src="../../../../assets/images/blogs/3.png" alt="img"></p>
<p>然后，我们来看三级页面如何爬取具体的标题和文章，文章的标题是<code>itemprop=”headline”</code>的<code>h1</code>标签的内容，文章的内容是<code>itemprop=”articleBody”</code>的<code>div</code>标签子节点中所有无<code>class</code>无<code>id</code>的<code>p</code>标签中内容。</p>
<p><img src="../../../../assets/images/blogs/4.png" alt="img"></p>
<p>以上对页面构建的分析透彻了之后，小白白写出了第一个爬虫代码，用了<code>beautifulsoup</code>类，它的<code>find/find_all</code>和<code>select</code>方法在之后我也用了好几次，返回值都是list类型，这点要稍微注意，访问时要像访问数组一样用<code>a[num]</code>或用<code>for…in</code>语句访问。两者的区别也是在用的过程中逐渐发现的，比如<code>find/find_all</code>可以获取指定<code>name</code>和<code>attr</code>的标签，如上文无<code>class</code>无<code>id</code>的<code>p</code>标签用<code>find</code>获取就很适合；<code>select</code>可以获取指定标签名和属性名的标签及他们的子孙节点，如二级页面中获取文章url需要寻找好几次子节点，用<code>select</code>就很简单。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line">-*- encoding: utf-8 -*-</div><div class="line"></div><div class="line">import re</div><div class="line">import urllib</div><div class="line">import  urllib2</div><div class="line">import sys</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding('utf8')</div><div class="line"></div><div class="line">	def getHtml(url):</div><div class="line">	    page = urllib.urlopen(url)</div><div class="line">	    html = page.read()</div><div class="line">	    return html</div><div class="line">	</div><div class="line">	def data_out(title,con,num):</div><div class="line">	    document = "file" + str(num) + ".txt"</div><div class="line">	    fo = open(document, "a+")</div><div class="line">	    string = title.encode('utf-8') + "\n" + con.encode('utf-8')</div><div class="line">	    fo.write(string)</div><div class="line">	    fo.close()  </div><div class="line">	</div><div class="line">	def getSecondURLs(html):</div><div class="line">	    secondURL = []    </div><div class="line">	    soup = BeautifulSoup(html,'lxml')</div><div class="line">	    # name = "div"</div><div class="line">	    # attrs = &#123;"class":"news_li"&#125;</div><div class="line">	    # ResultsTags = soup.find_all(name=name,attrs=attrs)</div><div class="line">	    ResultsTags = soup.select('div[class="box_list clearfix"] h2 a')</div><div class="line">	    for url in ResultsTags:</div><div class="line">	        Results = url.get("href")</div><div class="line">	        secondURL.append(Results)</div><div class="line">	    return secondURL</div><div class="line">	</div><div class="line">	def getSecondResult(urls,num):</div><div class="line">	    for url in urls:</div><div class="line">	          ContentTxt = ""</div><div class="line">	          html = getHtml(url)</div><div class="line">	          soup = BeautifulSoup(html)</div><div class="line">	          Title = (soup.find(name="h1",attrs=&#123;"itemprop":"headline","id":"artical_topic"&#125;))</div><div class="line">	          if not Title or not len(Title):</div><div class="line">	              Title = (soup.find(name="title"))</div><div class="line">	          if not Title or not len(Title):</div><div class="line">	              continue</div><div class="line">	          TitleTxt = [title for title in Title.strings][0]</div><div class="line">	          # 无标签文本</div><div class="line">	          # ContentTempTxt = [p.next_sibling.strip() for p in soup.findAll(name='div',attrs=&#123;"class":"contheight"&#125;)]</div><div class="line">	          ContentTemp = soup.select('div[id="main_content"]')</div><div class="line">	          if not ContentTemp or not len(ContentTemp):</div><div class="line">	              continue</div><div class="line">	          attrs = &#123;"class":False,"id":False&#125;		</div><div class="line">	          ContentTempTxt = ContentTemp[0].find_all(name="p",attrs=attrs)</div><div class="line">	          if not ContentTempTxt or not len(ContentTempTxt):</div><div class="line">	              continue</div><div class="line">	          for txt in ContentTempTxt:</div><div class="line">	              Results = txt.string</div><div class="line">	              if not Results:</div><div class="line">	                  continue</div><div class="line">	              ContentTxt = ContentTxt + str(Results)</div><div class="line">	          num = num + 1</div><div class="line">	          data_out(TitleTxt,ContentTxt,num)</div><div class="line">	      return num</div><div class="line">	</div><div class="line">	Page = 6256  #首页末页的Page值随着每天的不同会变化</div><div class="line">	filenum = 1051</div><div class="line">	while Page &lt; 6354:</div><div class="line">	CurrentURL = 'http://news.sohu.com/guojixinwen_' + str(Page) + '.shtml'</div><div class="line">	temp = getHtml(CurrentURL)</div><div class="line">	res = getSecondURLs(temp)</div><div class="line">	filenum = getSecondResult(res,filenum)</div><div class="line">	print Page</div><div class="line">	Page = Page + 1</div></pre></td></tr></table></figure></p>
<h5 id="（2）凤凰娱乐"><a href="#（2）凤凰娱乐" class="headerlink" title="（2）凤凰娱乐"></a>（2）凤凰娱乐</h5><p>接下来，小白白开始将程序改的更加自动化，比如，就某一类别，给定一个一级页面url后就不用管了，这里又找到了“凤凰娱乐”，也是一个很规范的页面</p>
<p><img src="../../../../assets/images/blogs/5.png" alt="img"></p>
<p>其中，明星、电影、电视、演出都有规则的文章列表，这样我们就从一级页面爬起，先从一级页面get到二级的url，再从二级页面get到三级页面（具体文章页面）的url，关键代码如下：<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">def getFirstURLs(html,num):</div><div class="line">    firstURL = []</div><div class="line">    soup = BeautifulSoup(html,'lxml')</div><div class="line">    number = 1</div><div class="line">    ResultsTags = soup.select('ul[class="clearfix"] li a')</div><div class="line">    for number in num:</div><div class="line">        Results = ResultsTags[number].get("href")</div><div class="line">        firstURL.append(Results)</div><div class="line">    return firstURL</div><div class="line"></div><div class="line">#其余函数基本与上面的相同</div><div class="line">#主程序</div><div class="line">filenum = 0</div><div class="line">InitialURL = 'http://ent.ifeng.com/'</div><div class="line">InitialHTML = getHtml(InitialURL)</div><div class="line">CurrentURLs = getFirstURLs(InitialHTML,[1,2,3,4])</div><div class="line">for CurrentURL in CurrentURLs:</div><div class="line">	PageNum = 1</div><div class="line">	url = CurrentURL    #CurrentURL是凤凰娱乐“明星”页第一页的url，url是当前准备要读取的某页的url</div><div class="line">	print CurrentURL</div><div class="line">	print filenum</div><div class="line">	for i in range(1,500):</div><div class="line">		temp = getHtml(url)</div><div class="line">		res = getSecondURLs(temp)  #获得的res是凤凰娱乐“明星”页的各具体文章的url</div><div class="line">		filenum = getSecondResult(res,filenum)		</div><div class="line">		print PageNum		</div><div class="line">		url = getNextPage(temp)</div><div class="line">		if not url:</div><div class="line">			break</div><div class="line">		PageNum = PageNum + 1</div></pre></td></tr></table></figure></p>
<p>至此，最简单的爬虫已经完成啦~当然因为我的数据量还是达不到要求，所以还需要继续奋斗。</p>
<h5 id="（3）新浪滚动新闻"><a href="#（3）新浪滚动新闻" class="headerlink" title="（3）新浪滚动新闻"></a>（3）新浪滚动新闻</h5><p>爬完了搜狐和凤凰所有可以爬的数据之后，语料库和预期的数据还是差了很远，接下来又开始了漫长的寻找数据源的过程，终于，一个庞大的数据源网站出现了——新浪新闻中心滚动新闻。通过往日回顾简直是要多少数据有多少数据，然而这里一个严峻的问题出现了，无论什么类别，网页源代码里始终只有一组同样的新闻标题，这说明，它背离了小白白最开始找源网页的原则，内容并没有写在html文件中，而是，写在了，JavaScript文件中！</p>
<p>然而这并没有阻碍小白白，经过一步步的探索，终于在chrome开发者模式network中发现网页加载了一个可疑的文件，将这个文件在新标签打开，还可以通过调整url的date和num值改变list中条数和内容。</p>
<p><img src="../../../../assets/images/blogs/6.png" alt="img"></p>
<p>接下来的问题就变成了如何从js文件中将js变量读取到python中，然而，并没有发现什么直接简便的方法，如果大牛们有简单方法，欢迎交流，迫不及待~~我觉得这简直是整个过程中的死穴…</p>
<p>那说说我的笨方法吧，由于没有找到js和python可以进行上述沟通的桥梁，我直接写了一段js把上面的变量中的url提取出来，形成了字符串型<code>list</code>，通过html调用js文件将<code>list</code>显示在了网页中。<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">var jsoncontent =[jsonData1,jsonData2,jsonData3,… …]  //还有很多天的数据</div><div class="line">var urlarr = [];</div><div class="line">var sum = 0;</div><div class="line">var max = 200;</div><div class="line">for(var j = 0; j&lt; max; j++)&#123;</div><div class="line">       for(var i = 0; i &lt; 10000; i++)&#123;</div><div class="line">              if(!jsoncontent[j].list[i])</div><div class="line">                     break;</div><div class="line">              else&#123;</div><div class="line">                     needpush =jsoncontent[j].list[i].url;</div><div class="line">                     needpush ="'"+needpush+"'";</div><div class="line">                     urlarr.push(needpush);</div><div class="line">              &#125;    </div><div class="line">              sum ++;</div><div class="line">       &#125;</div><div class="line">&#125;</div><div class="line">document.write(urlarr);</div></pre></td></tr></table></figure></p>
<p>其中<code>jsoncontent</code>是上面那个<code>http://roll.news....541</code>网页的内容，数据类型是list，因为有很多很多天的数据，每天是一个<code>jsonData</code>变量。为了针对这个roll.news巴拉巴拉的网页获取很多天的<code>jsonData</code>，小白白又写了一个爬虫，当然这个就比较简单啦，只是为了把网页的所有内容扒下来存储在一个文件里，然后直接复制粘贴到上面的js代码中就好了，爬虫大致就像下面这样。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">currentDate =date(<span class="number">2015</span>,<span class="number">6</span>,<span class="number">30</span>)</div><div class="line"><span class="selector-tag">i</span> = <span class="number">1</span></div><div class="line">whilecurrentDate<span class="selector-class">.month</span> != <span class="number">10</span> or currentDate<span class="selector-class">.day</span> != <span class="number">1</span>:</div><div class="line">       currentDate = currentDate +datetime.timedelta(days = <span class="number">1</span>)</div><div class="line">       urlDate = currentDate.strftime(<span class="string">'%Y-%m-%d'</span> )</div><div class="line">       currentURL = <span class="string">'http://roll.news.sina.com.cn/interface/rollnews_ch_out_interface.php?col=91&amp;spec=&amp;type=&amp;date='</span>\+urlDate+<span class="string">'&amp;ch=01&amp;k=&amp;offset_page=0&amp;offset_num=0&amp;num=1000&amp;asc=&amp;page=1&amp;r=0.8319043621666224'</span></div><div class="line">       jsData = getHtml(currentURL)</div><div class="line">       data_out(jsData,i)</div><div class="line">       <span class="selector-tag">i</span> = <span class="selector-tag">i</span> + <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>打开包含js文件的网页，得到了下面的样子，copy到下一步要编写的爬虫文件的urlsum变量中就可以啦</p>
<p><img src="../../../../assets/images/blogs/7.png" alt="img"></p>
<p>有了三级页面的url，爬虫就变得轻而易举了，重复上面的步骤，找到文章标题和内容的标签，改一下<code>getSecondResult()</code>函数，直接调用这个函数就好了，毕竟什么一级二级页面的工作前面都手动做完了~~<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">filenum = <span class="number">0</span></div><div class="line"><span class="selector-tag">i</span> = <span class="number">0</span></div><div class="line">maxnum = <span class="number">3501</span></div><div class="line">while <span class="selector-tag">i</span> &lt;maxnum :</div><div class="line">       filenum =getSecondResult(urlsum[i],filenum)</div><div class="line">       print <span class="string">"i = "</span> + str(i)</div><div class="line">       <span class="selector-tag">i</span> = <span class="selector-tag">i</span> + <span class="number">1</span></div></pre></td></tr></table></figure></p>
<p>有个point需要注意一下，我觉得应该是新浪有一定的反爬虫机制，爬大概10-20个页面就会报<code>IOError</code>，而且爬的速度明显比搜狐和凤凰慢很多，具体原因我也没搞明白，总之我在加了个异常处理，才让我的程序完整的跑完。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">try:</div><div class="line">     page = urllib.urlopen(url)</div><div class="line">except IOError:</div><div class="line">     print <span class="string">"Error:无法打开该网页"</span></div><div class="line">     return None</div><div class="line"><span class="keyword">else</span>:</div><div class="line">	<span class="selector-tag">html</span> = page.read()</div><div class="line">	return html</div></pre></td></tr></table></figure></p>
<p>爬完了要多少数据有多少数据的新浪滚动，反正我的每类两万是足够了，哦最后一个小问题，不要在限流量的地方爬虫，分分钟搞掉一个G，真的是每秒百兆啊，不说了，我去充网费了……</p>

                    
                        
                    
                    
                        <p>
                            <a href="/2016/12/14/一个编程小白白的爬虫&贝叶斯之旅（一）/#post-footer" class="postShorten-excerpt_link link">
                                注释和共享
                            </a>
                        </p>
                    
                </div>
            
        </div>
        
    </article>
    
    <div class="pagination-bar">
    <ul class="pagination">
        
        
        <li class="pagination-number">第 1 页 共 1 页</li>
    </ul>
</div>

</section>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2016 Lizany Li. All Rights Reserved.
    </span>
</footer>

            </div>
            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-remove"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/glass.jpg"/>
        
            <h4 id="about-card-name">Lizany Li</h4>
        
            <h5 id="about-card-bio"><p>一只青春后期的欢脱小白</p>
</h5>
        
        
            <h5 id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                <p>student</p>

            </h5>
        
        
            <h5 id="about-card-location">
                <i class="fa fa-map-marker"></i>
                <br/>
                Beijing
            </h5>
        
    </div>
</div>

        
<div id="algolia-search-modal" class="modal-container">
    <div class="modal">
        <div class="modal-header">
            <span class="close-button"><i class="fa fa-close"></i></span>
            <a href="https://algolia.com" target="_blank" class="searchby-algolia text-color-light link-unstyled">
                <span class="searchby-algolia-text text-color-light text-small">by</span>
                <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
            </a>
            <i class="search-icon fa fa-search"></i>
            <form id="algolia-search-form">
                <input type="text" id="algolia-search-input" name="search"
                    class="form-control input--large search-input" placeholder="Search "
                    autofocus="autofocus"/>
            </form>
        </div>
        <div class="modal-body">
            <div class="no-result text-color-light text-center">没有找到文章</div>
            <div class="results">
                
                <div class="media">
                    
                    <div class="media-body">
                        <a class="link-unstyled" href="https://lizanyroger.github.io/2016/12/14/一个编程小白白的爬虫&amp;贝叶斯之旅（一）/">
                            <h3 class="media-heading">一个编程小白白的爬虫&amp;贝叶斯之旅（一）</h3>
                        </a>
                        <span class="media-meta">
                            <span class="media-date text-small">
                                
                                    2016年12月14日
                                
                            </span>
                        </span>
                        <div class="media-content hide-xs font-merryweather"></div>
                    </div>
                    <div style="clear:both;"></div>
                    <hr>
                </div>
                
            </div>
        </div>
        <div class="modal-footer">
            <p class="results-count text-medium"
                data-message-zero="没有找到文章"
                data-message-one="找到 1 篇文章"
                data-message-other="找到 {n} 篇文章">
                找到 1 篇文章
            </p>
        </div>
    </div>
</div>
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        <!--SCRIPTS-->
<script src="/assets/js/script-taaobz4ueybn54ikf9gtu3wlm8tpgzjkwfknygrtitquljbwokv26p56moto.min.js"></script>
<!--SCRIPTS END-->



    <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.14.1/moment-with-locales.min.js"></script>
    <script src="//cdn.jsdelivr.net/algoliasearch/3/algoliasearch.min.js"></script>
    <script>
        var algoliaClient = algoliasearch('Z7A3XW4R2I', '12db1ad54372045549ef465881c17e743');
        var algoliaIndex = algoliaClient.initIndex('tranquilpeak');
    </script>

    </body>
</html>
